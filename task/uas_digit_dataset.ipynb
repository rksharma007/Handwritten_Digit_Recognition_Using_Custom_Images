{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ce080b2a-d508-473f-93f8-e34c96622194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c70a8786-146c-4bf6-a0b2-012914ae6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining path for train & test data\n",
    "PATH = os.getcwd()\n",
    "data_path = PATH + '/data'\n",
    "data_dir_list = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b912b94c-8ef4-47b6-80b6-e0fcb6be290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "#defining image size, channels and number of classes\n",
    "\n",
    "img_rows=28\n",
    "img_cols=28\n",
    "num_channel=1\n",
    "num_epoch=20\n",
    "num_classes = 10\n",
    "print(data_dir_list)   #displaying list in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2aa9a660-3c00-43bb-9bd7-26e9dee4b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-0\n",
      "\n",
      "Loaded the images of dataset-1\n",
      "\n",
      "Loaded the images of dataset-2\n",
      "\n",
      "Loaded the images of dataset-3\n",
      "\n",
      "Loaded the images of dataset-4\n",
      "\n",
      "Loaded the images of dataset-5\n",
      "\n",
      "Loaded the images of dataset-6\n",
      "\n",
      "Loaded the images of dataset-7\n",
      "\n",
      "Loaded the images of dataset-8\n",
      "\n",
      "Loaded the images of dataset-9\n",
      "\n",
      "(400, 28, 28)\n",
      "(400, 1, 28, 28)\n",
      "(400, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#some preprocessing of images\n",
    "img_data_list=[]\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "\timg_list=os.listdir(data_path+'/'+ dataset)\n",
    "\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "\tfor img in img_list:\n",
    "\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "\t\tinput_img_resize=cv2.resize(input_img,(28,28))\n",
    "\t\timg_data_list.append(input_img_resize)\n",
    "\n",
    "#converting images into numpy array \n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)\n",
    "img_data= np.expand_dims(img_data, axis=1)\n",
    "print (img_data.shape)\n",
    "img_data=np.rollaxis(img_data,3,1)\n",
    "img_data=np.rollaxis(img_data,3,1)\n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ff6ba362-1f68-4039-b050-46a0305da906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(320, 28, 28, 1)\n",
      "(320, 10)\n",
      "(80, 28, 28, 1)\n",
      "(80, 10)\n"
     ]
    }
   ],
   "source": [
    "#defining number of samples from shape of image data\n",
    "num_of_samples = img_data.shape[0]\n",
    "#creating labels for y_train and y_test\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:40]=0\n",
    "labels[40:80]=1\n",
    "labels[80:120]=2\n",
    "labels[120:160]=3\n",
    "labels[160:200]=4\n",
    "labels[200:240]=5\n",
    "labels[240:280]=6\n",
    "labels[280:320]=7\n",
    "labels[320:360]=8\n",
    "labels[360:]=9\n",
    "\n",
    "names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "input_shape=img_data[0].shape\t\n",
    "print(input_shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c747daee-dee5-42e2-bc61-612dbe6540be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28,1)\n",
    "\n",
    "#creating model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "27bb1b00-05c1-4009-a396-dacd35343151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compilatiion\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b028c3f4-c295-4a35-8418-a3b8601b97e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#printing summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f9b92860-1efb-44e1-8111-51b4a4f0e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "10/10 [==============================] - 1s 27ms/step - loss: 2.2461 - accuracy: 0.1625 - val_loss: 1.7540 - val_accuracy: 0.5875\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 2/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.5530 - accuracy: 0.4719 - val_loss: 1.0057 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 3/25\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0816 - accuracy: 0.6375 - val_loss: 0.6009 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 4/25\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8529 - accuracy: 0.7031 - val_loss: 0.5473 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 5/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6242 - accuracy: 0.7875 - val_loss: 0.4227 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 6/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.5387 - accuracy: 0.8125 - val_loss: 0.3343 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 7/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8562 - val_loss: 0.3617 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 8/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4536 - accuracy: 0.8375 - val_loss: 0.3228 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 9/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.4120 - accuracy: 0.8438 - val_loss: 0.2686 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 10/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3797 - accuracy: 0.8781 - val_loss: 0.2413 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00010: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 11/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3904 - accuracy: 0.8750 - val_loss: 0.2845 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00011: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 12/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3046 - accuracy: 0.8875 - val_loss: 0.2602 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00012: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 13/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3095 - accuracy: 0.8875 - val_loss: 0.2115 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00013: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 14/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1988 - accuracy: 0.9250 - val_loss: 0.1816 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00014: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 15/25\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.1897 - accuracy: 0.9375 - val_loss: 0.2223 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00015: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 16/25\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.2409 - accuracy: 0.9187 - val_loss: 0.1463 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00016: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 17/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1821 - accuracy: 0.9312 - val_loss: 0.2284 - val_accuracy: 0.9625\n",
      "\n",
      "Epoch 00017: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 18/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1928 - accuracy: 0.9125 - val_loss: 0.1482 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00018: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 19/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1949 - accuracy: 0.9344 - val_loss: 0.2100 - val_accuracy: 0.9625\n",
      "\n",
      "Epoch 00019: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 20/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1740 - accuracy: 0.9281 - val_loss: 0.1946 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00020: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 21/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.2079 - accuracy: 0.9250 - val_loss: 0.1812 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00021: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 22/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1769 - accuracy: 0.9375 - val_loss: 0.2075 - val_accuracy: 0.9250\n",
      "\n",
      "Epoch 00022: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 23/25\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1646 - accuracy: 0.9187 - val_loss: 0.1422 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00023: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 24/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1222 - accuracy: 0.9594 - val_loss: 0.1538 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00024: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "Epoch 25/25\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1199 - accuracy: 0.9406 - val_loss: 0.1494 - val_accuracy: 0.9750\n",
      "\n",
      "Epoch 00025: saving model to C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\n",
      "INFO:tensorflow:Assets written to: C:/Users/theycallmeBOT/Desktop/task/checkpoint\\cp.ckpt\\assets\n",
      "The model has successfully trained\n",
      "Saving the model as uas_digit_dataset_model.h5\n"
     ]
    }
   ],
   "source": [
    "#defining path to save checkpoints\n",
    "checkpoint_path = 'C:/Users/theycallmeBOT/Desktop/task/checkpoint/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#creating checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "#starting model training\n",
    "hist = model.fit(X_train, y_train, batch_size=32, epochs=25, verbose=1, validation_data=(X_test, y_test), callbacks=[cp_callback])\n",
    "print(\"The model has successfully trained\")\n",
    "\n",
    "#saving trained model\n",
    "model.save('uas_digit_dataset_model.h5')\n",
    "print(\"Saving the model as uas_digit_dataset_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d6f6a8ad-92af-4ce1-a059-defc61849735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.14936895668506622\n",
      "Test accuracy: 0.9750000238418579\n"
     ]
    }
   ],
   "source": [
    "#printing validation loss and accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5ab1e5b8-08f1-4e5f-8ded-8837ce3ecc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(img):\n",
    "    #resize image to 28x28 pixels\n",
    "    img = img.resize((28,28))\n",
    "    #convert rgb to grayscale\n",
    "    img = img.convert('L')\n",
    "    #inverting image\n",
    "    #img = PIL.ImageOps.invert(img)\n",
    "    img = np.array(img)\n",
    "    #reshaping to support our model input and normalizing\n",
    "    img = img.reshape(1,28,28,1)\n",
    "    img = img/255.0\n",
    "    #predicting the class\n",
    "    res = model.predict([img])[0]\n",
    "    print('\\nPredicted Digit:  ', np.argmax(res) , ' \\nPrediction Accuracy:', round(100*max(res), 4), '%')\n",
    "    #return np.argmax(res), 100*max(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "94ab3c7b-39bb-4a03-bfcc-54e7cf5b64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Digit:   4  \n",
      "Prediction Accuracy: 99.9924 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path1 = 'C:/Users/theycallmeBOT/Desktop/task/my_handwritten_digits/four2-01.png'\n",
    "img_path2 = 'C:/Users/theycallmeBOT/Desktop/task/data/9/9_32.png'\n",
    "test_image = Image.open(img_path1)\n",
    "#test_image.show()\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "predict_digit(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b1225-24f1-498a-97ff-181591b826da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe50d5a-3563-4a6b-8082-18cf6ef30004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
